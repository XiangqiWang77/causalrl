
# CausalRL: Causal-Enhanced Policy Optimization (CE-PO)

This repository contains the official implementation of **Causal-Enhanced Policy Optimization (CE-PO)**. 

CE-PO is a model-agnostic reward fusion framework designed to mitigate "shortcut learning" in Large Language Models (LLMs). It augments standard policy optimization (PPO/GRPO) by combining task accuracy rewards with **Jacobian-based causal coherence signals**. [cite_start]This ensures that the model's reasoning pathway ($Z \rightarrow X \rightarrow Y$) is not only accurate but faithful

---

## ðŸ“‚ Project Structure

[cite_start]The project is organized to separate the causal reward calculation from the policy optimization loop, leveraging the **Verl** framework[cite: 356].

```text
CausalRL/
â”œâ”€â”€ reward_part/        # Jacobian Reward Calculation
â”‚   â”œâ”€â”€ run.sh          # Script to generate/calculate causal signals
â”‚   â””â”€â”€ ...             # Logic for counterfactual hardening & spectral normalization
â”œâ”€â”€ trainer/            # Policy Optimization (Verl-based)
â”‚   â”œâ”€â”€ llama38.sh      # Main training script (e.g., Llama-3-8B)
â”‚   â””â”€â”€ ...             # PPO/GRPO implementation with Minkowski combiner
â”œâ”€â”€ data/               # Academic datasets (BBEH, CaseHOLD, etc.)
â”œâ”€â”€ checkpoints/        # Saved models and reward checkpoints
â””â”€â”€ logs/               # Training logs
```

---

## âš¡ Quick Start

### Prerequisites

The code relies on the **Verl** framework for efficient RLHF training. Ensure your environment is set up with Python 3.11+.

### 1. Activate Jacobian Reward Calculation

CE-PO requires differentiable, model-internal proxies for causal coherence. You must first compute these signals or run the reward server thread.

```bash
cd reward_part
bash run.sh

```

This step computes the raw Jacobian sensitivities and applies **counterfactual hardening** to remove nuisance factors (e.g., length biases).

### 2. Run Policy Optimization (CE-PO)

Once the reward signals are available, run the training script. The trainer uses a **Minkowski (power-mean) combiner** to fuse the accuracy reward (e.g., BERTScore) with the causal signals.

```bash
cd ..
bash trainer/llama38.sh

```

> 
> **Note:** The training uses **PPO** or **GRPO** (Group Relative Policy Optimization) as the underlying RL algorithm.
> 
> 

---

## Parallel Computing Implementation

A critical implementation detail of CausalRL is the handling of computational overhead. Calculating Jacobian matrices for every token generation is computationally intensive.

To address this, this implementation modifies the base **Verl** framework to utilize a **separate thread for reward calculation**.

* **Main Thread:** Handles the policy optimization (actor/critic updates).
* 
**Reward Thread:** Asynchronously computes the Jacobian-based causal influence scores (, ) and performs the spectral energy normalization.



---

## Dataset Sources

The repository includes support for the following academic datasets used in the paper's experiments to evaluate reasoning faithfulness and robustness.

Training & Validation Sets 

* **BBEHCausal:** The causal reasoning split from the BIG-Bench Extra Hard dataset.


* **CaseHOLD:** Multiple-choice legal holdings requiring context from case citations.


* **MATHHARD:** A subset of the MATH dataset focusing on multi-step reasoning (Level 5).


* **IfQA:** Open-domain QA requiring hypothetical reasoning under counterfactual presuppositions.



Testing & OOD (Out-of-Distribution) Sets 

* **BBEHMATH:** Challenging multistep arithmetic tasks.


* **CLadder:** A benchmark for association, intervention, and counterfactual queries.


* **LegalBench:** Case-understanding split for analyzing legal reasoning.


* **LogiQA:** Deductive logical reasoning from officer entrance exams.


* **TruthfulQA / CodeMMLU / SuperGPQA:** Used for evaluating generalization and robustness.



```
